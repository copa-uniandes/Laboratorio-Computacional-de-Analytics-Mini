{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![miad4.png](Archivos/miad4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Por precaución, cambiamos el directorio activo de Python a aquel que contenga este notebook\n",
    "if \"PAD-book\" in os.listdir():\n",
    "    os.chdir(r\"PAD-book/Laboratorio-Computacional-de-Analytics/S3 - Librerias para manejo de datos/S3.LE1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Cuál es el flujo de trabajo para analítica de datos en Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta el momento, hemos cubierto el uso de Python en su expresión más básica; es decir, considerando únicamente sus estructuras de datos nativas, así como las estructuras de control de flujo. Si bien a partir de esa base es posible lograr casi cualquier desarrollo, una de las ventajas de un lenguaje *open source* es que la comunidad desarrolla librerías de código especializado para suplir necesidades en áreas de aplicación específicas. El análisis de datos, lejos de ser la excepción, es un área en la que continuamente se están desarrollando y actualizando librerías concebidas para facilitar la labor del analista. Así, la práctica estándar es partir de librerías que proveen código precocido para resolver tareas comunes y no tener que programar todo desarrollo desde cero.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta ocasión haremos un preámbulo sobre el propósito de las librerías `numpy` y `pandas` como pilar del análisis de datos en Python, enfatizando en la versatilidad que ofrecen al tener gran cantidad de métodos útiles incorporados y de acoplarse fácilmente con otras librerías especializadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías populares para el análisis de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las librerías más populares y útiles en Python es `numpy`, la cual permite desarrollar operaciones numéricas de forma fácil y eficiente. Esta librería introduce la estructura de datos `array`, similar a las listas, pero con soporte para operaciones como la suma término a término (que no existe en las listas), multiplicaciones de matrices y análisis numérico en general (algebra lineal, probabilidad). Por lo anterior, `numpy` es una librería ampliamente usada el contexto de computación científica y también es responsable de mucha de la computación en análisis de datos, ya que, como veremos después, varias librerías especializadas se basan en `numpy`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede decir que la librería central para análisis de datos en Python es `pandas`, ya que facilita la representación de datos con base en estructuras de datos convenientes, acompañadas de métodos que permiten numerosas operaciones sobre ellos. Estas estructuras de datos básicas se llaman `Series` y `DataFrame`. Como su nombre lo indica, un objeto de la clase `Series` corresponde a secuencia de datos, similar a las listas, pero permite indexación diferente al orden; por ejemplo, para indexar los datos se pueden utilizar fechas, etiquetas de texto, entre otros. Adicionalmente, un `Series` soportan una variedad de operaciones similares a las que se aplicarían a columnas de una hoja de cálculo, como veremos cuando hablemos de los `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estructura de datos más usada en *analytics* es el `DataFrame`, que puede ser interpretado como una colección de `Series`. Un `DataFrame` es análogo a una hoja de cálculo, en la que se tienen columnas (`Series`) con datos de diferentes tipos, cada una identificada con un nombre o índice, con una variedad de métodos disponibles que permiten hacer operaciones cotidianas de análisis de datos. Algunas de las operaciones que se pueden realizar con un `DataFrame` son: calcular sumas, promedios, máximos, mínimos, o incluso generar gráficas, entre muchas otras. Se hace tan trivialmente como se haría en una hoja de cálculo, invocando el nombre de una función y, en algunos casos, especificando ciertos parámetros de entrada. A continuación veremos un ejemplo de cómo cargar un `DataFrame` existente en la librería `seaborn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "print(type(iris))\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, cabría la pregunta: ¿por qué usaría `pandas` (teniendo en cuenta que implica programar) si hace lo mismo que una hoja de cálculo?. Una primera respuesta es que las instrucciones que utilizamos en hojas de cálculo son, en efecto, una forma básica de programación. Precisamente por eso, usar una herramienta como `pandas` no debería resultar muy complicado. Sin embargo, sí existen ventajas objetivas de `pandas` sobre las hojas de cálculo:<br><br>\n",
    "\n",
    "* El desempeño computacional superior que ofrece `pandas`, permitiendo la manipulación de conjuntos de datos más grandes.<br><br>\n",
    "\n",
    "* La facilidad para pasar de `pandas` a otras librerías especializadas de análisis de datos que ofrecen herramientas más allá del alcance de hojas de cálculo (ej., `sklearn`, `tensorflow`).<br><br>\n",
    "\n",
    "* La versatilidad de `pandas` para importar y exportar datos en una amplia variedad de formatos, permitiendo agregar datos desde archivos `.json`, `.html`, `.xls`, bases de datos, entre muchos otros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este módulo del curso es lograr importar datos con `pandas` desde cualquier formato para guardarlos como `DataFrame` o `Series`, allí explorarlos y manipularlos utilizando los métodos disponibles, así como algunas librerías adicionales de exploración visual. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análisis exploratorio es todo: ahí es donde surgen y se perfeccionan las preguntas. Al hacer análisis exploratorio es muy frecuente preguntarse si alguna variable o algún valor específico que tome una variable da lugar a algún comportamiento de interés. A continuación veremos algunos de estos casos, relacionándolos con las habilidades de programación necesarias para llevar a cabo los análisis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el contexto científico, es muy común ver estudios clínicos o económicos en los que se pregunta sobre el impacto de algún medicamento o alguna política en diferentes poblaciones. Por ejemplo, se quiere saber si cierto tratamiento tiene un mayor efecto en mujeres que en hombres o si las personas con cierto nivel educativo perciben mayores ingresos. Al momento de presentar conclusiones definitivas es necesario llevar a cabo pruebas estadísticas formales que confirmen la existencia de los efectos evaluados. Sin embargo, estas preguntas no se formulan a la ligera, sino que provienen de pasos previos de análisis exploratorio. En el contexto de *analytics* en las organizaciones, esta exploración de preguntas de interés parte del conocimiento del negocio y el análisis descriptivo de los datos. Las estrategias más ampliamente utilizadas para este propósito son los descriptivos generales de las variables de interés (que hemos cubierto previamente), los filtros y análisis por grupos (tema de esta lectura) y el análisis visual (que cubriremos posteriormente). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado y agrupado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El filtrado de datos es una de las prácticas más comunes en análisis y está directamente relacionado con las estructuras de flujo condicionales. En esencia, queremos observar un subconjunto de los datos que cumpla con una o más condiciones. Por ejemplo, en una base de datos del personal de una organización, podríamos preguntarnos por las condiciones de quienes están próximos a pensionarse. En este caso, querríamos concentrarnos en aquellos registros (filas) en los que la variable o columna correspondiente a la edad tome un valor por encima de cierta edad. Si lo pensamos en términos de programación básica en Python, esto implicaría tener que ejecutar un ciclo (`for` o `while`) que recorra todas las filas y en cada caso ejecute un condicional (`if`) que se pregunte si la edad de la persona actual es mayor o igual a la edad especificada para considerarse próximo a pensionarse. Lo anterior tiene como desventaja que habría que escribir explícitamente el código, lo cual demanda tiempo, esfuerzo y viene con una alta probabilidad de que el código escrito no sea óptimo en términos de eficiencia computacional. Por esta razón, `pandas` dispone de una sintaxis que permite hacer filtrados complejos eficientemente sin necesidad de programar explícitamente los ciclos y condicionales necesarios. Esta funcionalidad se basa en la indexación lógica de estructuras de datos que estudiamos en los casos de los `array` de `numpy`, en los cuales usamos condiciones de verdadero y falso para seleccionar las posiciones de una estructura de datos que queríamos ver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de los `DataFrame` y `Series` podemos aplicar los mismos conceptos. Podemos imponer condiciones en una o más de las columnas de un conjunto de datos y concentrarnos solo en los casos que nos interesan, valiéndonos de los operadores `|`, `&`, `~`, entre otros operadores lógicos, como `all` o `any`, para imponer múltiples condiciones. Por ejemplo, podría interesarnos seleccionar los datos correspondientes a empleados que tengan una edad mayor a cierto valor y menor a cierto valor; aquellos que estén inscritos en un cierto plan de pensiones u otro; o una combinación de ambas. Al ser capaces de extraer rápidamente aquellos datos que cumplen con ciertos criterios, podemos aplicar los análisis descriptivos básicos que conocemos para responder preguntas exploratorias más interesantes. Por ejemplo: ¿será cierto que el salario promedio de quienes están próximos a pensionarse es mayor que el del resto de empleados? ¿será cierto que el promedio de ventas de un empleado es menor cuando está próximo a pensionarse? El hecho de filtrar los datos ofrece la posibilidad de hacer análisis comparativos para generar y probar hipótesis que pueden dar lugar a preguntas de negocio de interés para la toma de decisiones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análisis por grupos es una funcionalidad más avanzada que parte de un principio o motivación similar: ¿existe alguna diferencia en los datos que corresponden a cierto grupo o categoría? El método de `pandas` en el que nos concentraremos para este tipo de análisis es `groupby`. La diferencia fundamental es que, mientras en el filtrado imponíamos una o más condiciones sobre el valor de una variable o columna (ej., seleccionar personas en un rango de edad específico), en el análisis por grupos queremos observar o calcular métricas específicas para todos los segmentos en que una o más variables categóricas dividan los datos. Esto es, si los datos contienen una variable categórica relacionada con la `\"Ciudad\"`, lo que queremos ver es cómo varía el comportamiento de los datos según todos los posibles valores que tome la variable `\"Ciudad\"`. Para enfatizar la diferencia, pensemos que lo que haríamos con el filtrado sería obtener el subconjunto de datos en los cuáles la variable `\"Ciudad\"` sea igual a cierta ciudad o incluso a una de varias ciudades:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Igual a una ciudad\n",
    "df[df[\"Ciudad\"] == \"Bogotá\"]\n",
    "\n",
    "# Igual a una de varias ciudades\n",
    "df[df[\"Ciudad\"] == \"Bogotá\" | df[\"Ciudad\"] == \"Lima\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cualquier caso el resultado es un único conjunto de datos que cumplen con la condición impuesta. Por el contrario, al hacer análisis por grupos, lo que obtendríamos son múltiples conjuntos de datos: uno por cada valor de la variable `\"Ciudad\"`, de forma que todos los datos que corresponden a `\"Lima\"` han sido seleccionados de forma separada a los que corresponden a `\"Bogotá\"` y a los que corresponden a `\"Buenos Aires\"`. La mayor ventaja de esto es que, una vez hemos aplicado el agrupamiento de datos, podemos calcular fácilmente métricas sobre los conjuntos separados, de forma que podemos hacer análisis comparativos más elaborados que los que permite el filtrado básico. Por ejemplo, si aplicamos el método para calcular el promedio (`mean`) o la desviación (`std`) sobre los datos agrupados tendremos, no un único promedio, sino los promedios independientes de los datos que correspondan a cada ciudad. Con esto, resultará muy sencillo evaluar, por ejemplo, si el promedio de los precios de los restaurantes en `\"Bogotá\"` es menor, pero tiene mayor desviación que las demás ciudades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por supuesto, tanto el filtrado como el agrupamiento pueden implementarse desde cero en Python; incluso, es posible implementar agrupamiento a través de un código que aplique múltiples filtrados. Sin embargo, la sintaxis y métodos que ofrece `pandas` facilita bastante la implementación de estos análisis. Esto agrega mucho valor al proceso de *analytics* porque dichos análisis típicamente ocurren en una dinámica de ensayo y error, en la que estamos probando diferentes hipótesis antes de llegar a un hallazgo de más trascendencia para abordar problemas de negocio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créditos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Autores__: Camilo Hernando Gómez Castro, Alejandro Mantilla Redondo, Diego Alejandro Cely Gómez\n",
    " \n",
    "__Fecha última actualización__: 07/07/2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
