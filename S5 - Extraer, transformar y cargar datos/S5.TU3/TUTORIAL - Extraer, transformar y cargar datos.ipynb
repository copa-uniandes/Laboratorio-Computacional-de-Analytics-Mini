{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![miad4.png](Archivos/miad4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Por precaución, cambiamos el directorio activo de Python a aquel que contenga este notebook\n",
    "if \"PAD-book\" in os.listdir():\n",
    "    os.chdir(r\"PAD-book/Laboratorio-Computacional-de-Analytics/S5 - Extraer, transformar y cargar datos/S5.TU3/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción, transformción y carga de datos: `pyspark`\n",
    "\n",
    "<!--# ETL y principios de _Big Data_ ????-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraer, transformar y cargar (ETL por sus siglas en inglés), describe un proceso en tres etapas:\n",
    "\n",
    "1. obtener datos de una o más fuentes; <br>\n",
    "2. transformar los datos, sea haciéndoles limpieza, combinándolos o añadiendo registros; <br>\n",
    "3. grabar los datos, sea cargándolos a su misma fuente, persistiéndolos en archivos locales o alimentando consumidores que dependen de nuestros resultados (aplicaciones _downstream_).\n",
    "\n",
    "Hasta ahora hemos tratado fuentes de datos almacenadas localmente en archivos de texto o formatos de herramientas estadísticas como Stata. La realidad es que los datos existen en un sinfín de contextos, casi siempre, en formatos altamente especializados que exigen procesos especializados de extracción, como SQL (_Structured Query Language_) para bases de datos relacionales. También nos hemos enfocado solo en consumir los datos sin preocuparnos mucho acerca de cómo podemos hacer nuestros resultados disponibles en ocasiones futuras o para otros usuarios.\n",
    "\n",
    "Las bases de datos, como las hemos trabajado, existen únicamente en la memoria volátil de nuestro computador y no persisten de una sesión de Python a la siguiente. Podemos hacer persistir los cambios si los grabamos en archivos tipo `.txt` o `.csv`, pero esto no es manejable a escala o, por ejemplo, cuando queremos desagregar los datos y distribuirlos en distintas tablas para que sean compatibles con operaciones del álgebra relacional (en esencia, lo que hacen las funciones `join` o `merge` de `pandas`).\n",
    "\n",
    "En este tutorial exploraremos el uso de la librería `pyspark` para el manejo de bases de datos relacionales, en el contexto de procesos de ETL para analítica de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requisitos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para desarrollar este tutorial necesitarás:\n",
    "\n",
    "* Importar y exportar archivos de texto en formato `.txt` o `.csv` por medio de un *file handle*. <br>\n",
    "* Utilizar operaciones sencillas y vectorizadas en `numpy` y `pandas`. <br>\n",
    "* Crear, consultar y utilizar métodos para explorar y manipular objetos tipo `DataFrame` en `pandas`. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al final de este tutorial podrás:\n",
    "\n",
    "**1.** Distinguir situaciones en las que resulta más beneficioso utilizar herramientas de ETL como `pyspark`.<br>\n",
    "**2.** Reconocer estructuras de datos que distribuyen la carga de sus operaciones en procesos paralelos.<br>\n",
    "**3.** Extraer y transformar tablas de bases de datos relacionales con operaciones de algebra relacional. <br>\n",
    "**4.** Crear y cargar tablas a bases de datos relacionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Entorno de desarrollo en Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fundación de _software_ Apache es una comunidad dedicada al desarrollo de herramientas _open source_, entre estas, Spark: un motor de procesamiento de datos altamente eficiente. Spark está diseñado para desplegarse en entornos de cómputo distribuido (_cluster_) y paraleliza sus operaciones de manera implícita, lo que lo hace ideal para el procesamiento de volúmenes altos de datos (_Big Data_). Cualquier aplicación puede llegar a imlementar Spark en su flujo de datos por medio de sus APIs (_Application Program Interface_) para distintos lenguajes de programación, como `pyspark` para Python o `SparkR` para R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así como Spark es una aplicación externa a Python a la que accedemos por medio de su API, debemos importar la API, configurar la sesión y luego inicializar la aplicación. Nos enfocaremos en el módulo `sql` para procesos de ETL con bases de datos relacionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/12 09:54:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://alejoman-pc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Instancia_Tutorial_PySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe9499cf490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos el metodo SparkSession para configurar e inicializar una instancia de Spark SQL\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializamos la aplicacion\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[*]\") \\\n",
    "      .appName(\"Instancia_Tutorial_PySpark\") \\\n",
    "      .getOrCreate()\n",
    "\n",
    "# Imprimimos el objeto que contiene la sesión\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `master` nos permite definir el contexto de paralelización de los procesos. Veamos algunos parámetros que puede recibir:\n",
    "\n",
    "* `\"local[n]\"` le indica a la aplicación que debe ejecutarse en el mismo computador, con `n` cantidad de procesos para paralelizar las tareas; <br><br>\n",
    "\n",
    "* `\"Yarn\"` (u otro nombre de un administrador de _clusters_), le indican a la aplicación que debe ejecutarse en un _cluster_, el cual tendríamos que configurar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al hacer clic en el enlace del _output_ de la celda (Spark UI), podrás acceder a una interfaz web, igual a la de la imágen, con detalles de la aplicación. Esto funciona solo si expones a la red el puerto de la aplicación o si ejecutas el programa en tu propio computador, ya que necesitas acceso al servicio web desplegado por Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark_GUI.png](Archivos/Spark_GUI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estructuras de datos en `pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar la ejecución de tareas en paralelo, Spark incluye dos estructuras de datos que permiten distribuir sus operaciones sobre distintos objetos en memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 RDDs (_Resilient Distributed Dataset_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un RDD es la representación de una colección de objetos que pueden distribuirse en distintos procesos o incluso almacenarse en distintos nodos de nuestro _cluster_. Resultan bastante útiles y eficientes para procesar grandes cantidades de datos, pero resultan poco convenientes si nuestra intención es hacer cambios sobre los datos ya que son inmutables (una vez creados, no pueden editarse).\n",
    "\n",
    "Esta representación es poco restrictiva en cuanto a qué consideramos un objeto de la colección. Para nuestras intenciones, podríamos pensar en un RDD como un `DataFrame` de `pandas` cuyas filas hemos almacenado en distintas variables. Alternativamente, podríamos particionar sobre las columnas, en cuyo caso tendríamos una colección de objetos tipo `Series` o, si una partición toma más de una columna, una colección de objetos tipo `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos declarar un RDD a partir de diferentes estructuras de datos. A continuación, vemos un ejemplo de cómo declarar a partir de una lista de listas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = [[\"Programa_1\", 2000, 4500, \"LAN\"],\n",
    "             [\"Programa_2\", 3401, 7000, \"LAS\"],\n",
    "             [\"Programa_3\", 50,   7000, \"LAS\"],\n",
    "             [\"Programa_4\", 7850, 3300, \"USW\"],\n",
    "             [\"Programa_5\", 8000, 3505, \"LAN\"]]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data_list)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota que al intentar imprimir el RDD, nos encontramos con que la representación en el _output_ de la celda no muestra el contenido. Esto se debe a que las operaciones sobre los RDD son _lazy_ (perezosas) y no se ejecutarán hasta que sea absolutamente necesario. Este esquema de ejecución diferida permite optimizar de manera anticipada las consultas y operaciones para minimizar el tiempo de ejecución y la ocupación de la memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operaciones y consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ahora, basta con saber que hay acciones y transformaciones. Bajo ninguna circunstancia estas operaciones modificarán el RDD original; en su lugar, Spark crea una copia del RDD con los cambios necesarios una vez se hayan realizado las operaciones cargadas de manera perezosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veámos a continuación un ejemplo de cómo operar sobre los elementos de la colección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declaramos una funcion que tome los elementos con indice 1 y 2 de un iterable y los multiplique.\n",
    "def mult(x):\n",
    "    return x[1] * x[2]\n",
    "\n",
    "# La transformacion map toma cada objeto contenido en el RDD y ejecuta la funcion que recibe por parametro.\n",
    "nuevo_rdd = rdd.map(mult)\n",
    "nuevo_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `nuevo_rdd` contiene un RDD distinto a `rdd` para el cual aún no se ha efectuado la transformación. Hacemos uso del método `collect` para ejecutar las operaciones pendientes y recolectar los resultados en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Programa_1', 2000, 4500, 'LAN'],\n",
       " ['Programa_2', 3401, 7000, 'LAS'],\n",
       " ['Programa_3', 50, 7000, 'LAS'],\n",
       " ['Programa_4', 7850, 3300, 'USW'],\n",
       " ['Programa_5', 8000, 3505, 'LAN']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9000000, 23807000, 350000, 25905000, 28040000]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nuevo_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tener que recolectar los objetos de RDD resulta útil en escenarios donde debemos procesar nuestros datos en distintas etapas y no nos interesa el resultado de operaciones intermedias. La ejecución perezosa de las transformaciones nos permite concatenarlas sin ocupar memoria de manera redundante entre operaciones y optimiza de manera automática e inteligente las instrucciones para no repetir tareas redundantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Objeto `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar a los `DataFrame` de `pandas`, los `DataFrame` en `pyspark` son una colección mutable de datos organizados por columnas. A diferencia de `pandas`, que opera de manera secuencial, la implementación de `pyspark` almacena las filas de manera distribuida y opera sobre ellas de manera paralelizada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos declarar un `DataFrame` a partir de diferentes estructuras de datos. A continuación, vemos un ejemplo de cómo declarar a partir de una lista de listas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Nombre: string, Descargas: bigint, Lineas_de_codigo: bigint, Region: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"Nombre\", \"Descargas\", \"Lineas_de_codigo\", \"Region\"]\n",
    "df = spark.createDataFrame(data=data_list, schema=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro `schema` (esquema) hace referencia a los campos de una estructura de datos. En el caso de nuestra lista de listas, el esquema es el nombre de las columnas. Si tuvieramos un diccionario de listas, el esquema serían sus llaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyspark` incluye métodos dedicados a cargar datos desde una gran variedad de formatos. Consideremos, por lo pronto, un format con el que estemos familiarizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- medios_noti_redessociales: string (nullable = true)\n",
      " |-- medios_noti_chat: string (nullable = true)\n",
      " |-- medios_noti_periodicos: string (nullable = true)\n",
      " |-- medios_noti_tv: string (nullable = true)\n",
      " |-- medios_noti_radio: string (nullable = true)\n",
      " |-- medios_covid_redessociales: string (nullable = true)\n",
      " |-- medios_covid_chat: string (nullable = true)\n",
      " |-- medios_covid_periodicos: string (nullable = true)\n",
      " |-- medios_covid_tv: string (nullable = true)\n",
      " |-- medios_covid_radio: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_covid_19 = spark.read.option(\"header\",\"true\").csv(\"Archivos/BID-Cornell.csv\")\n",
    "df_covid_19.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operaciones y consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos distintas alternativas para consultar el contenido de nuestros `DataFrame`. Podemos utilizar el método `show` para imprimir de manera estilizada la tabla o los métodos `head` y `tail` para retornar las primeras o últimas filas de la tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------------+------+\n",
      "|    Nombre|Descargas|Lineas_de_codigo|Region|\n",
      "+----------+---------+----------------+------+\n",
      "|Programa_1|     2000|            4500|   LAN|\n",
      "|Programa_2|     3401|            7000|   LAS|\n",
      "|Programa_3|       50|            7000|   LAS|\n",
      "|Programa_4|     7850|            3300|   USW|\n",
      "|Programa_5|     8000|            3505|   LAN|\n",
      "+----------+---------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Nombre='Programa_1', Descargas=2000, Lineas_de_codigo=4500, Region='LAN'),\n",
       " Row(Nombre='Programa_2', Descargas=3401, Lineas_de_codigo=7000, Region='LAS'),\n",
       " Row(Nombre='Programa_3', Descargas=50, Lineas_de_codigo=7000, Region='LAS')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Nombre='Programa_4', Descargas=7850, Lineas_de_codigo=3300, Region='USW'),\n",
       " Row(Nombre='Programa_5', Descargas=8000, Lineas_de_codigo=3505, Region='LAN')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabe notar que por medio de los métodos `head` y `tail`, obtenemos objetos de tipo `Row` que son los que ditribuye Spark para paralelizar las operaciones. El método `limit` nos permite persitir el tipo `DataFrame` como resultado de la consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------------+------+\n",
      "|    Nombre|Descargas|Lineas_de_codigo|Region|\n",
      "+----------+---------+----------------+------+\n",
      "|Programa_1|     2000|            4500|   LAN|\n",
      "|Programa_2|     3401|            7000|   LAS|\n",
      "|Programa_3|       50|            7000|   LAS|\n",
      "+----------+---------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ocasiones puede ser deseable convertir entre los `DataFrame` de `pyspark` a su contraparte de `pandas`. Transformar un `DataFrame` de `pyspark` a `pandas` es solo recomendable si se cuenta con suficiente almacenamiento o memoria. Los altos volúmenes de datos que manejamos en `pyspark` con gran eficiencia en memoria y procesamiento pueden no ser compatibles con otras librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Descargas</th>\n",
       "      <th>Lineas_de_codigo</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Programa_1</td>\n",
       "      <td>2000</td>\n",
       "      <td>4500</td>\n",
       "      <td>LAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Programa_2</td>\n",
       "      <td>3401</td>\n",
       "      <td>7000</td>\n",
       "      <td>LAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Programa_3</td>\n",
       "      <td>50</td>\n",
       "      <td>7000</td>\n",
       "      <td>LAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Programa_4</td>\n",
       "      <td>7850</td>\n",
       "      <td>3300</td>\n",
       "      <td>USW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Programa_5</td>\n",
       "      <td>8000</td>\n",
       "      <td>3505</td>\n",
       "      <td>LAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Nombre  Descargas  Lineas_de_codigo Region\n",
       "0  Programa_1       2000              4500    LAN\n",
       "1  Programa_2       3401              7000    LAS\n",
       "2  Programa_3         50              7000    LAS\n",
       "3  Programa_4       7850              3300    USW\n",
       "4  Programa_5       8000              3505    LAN"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = df.toPandas()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operaciones y consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera similar a los RDD, podemos operar sobre los registros de un `DataFrame` de Spark con funciones nativas o por medio funciones definidas por el usuario. El método `withColumn` nos permite agregar o reemplazar una columna a un `DataFrame`, poblándola registro a registro. El resultado es un nuevo `DataFrame` que refleja estos cambios.\n",
    "\n",
    "```python\n",
    "df.withColumn(colName:str, col:Column)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro `colName` permite definir el nombre que tendrá la columna en el nuevo `DataFrame`. El parámetro `col` recibe una expresión en función de las columnas existentes a partir de la cual `pyspark` calcula el valor para cada registro de la columna `colName`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la función `col` para crear objetos a partir de los cuales crear la expresión del parámetro `col`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------------+------+----------------+\n",
      "|    Nombre|Descargas|Lineas_de_codigo|Region|DescargasXLineas|\n",
      "+----------+---------+----------------+------+----------------+\n",
      "|Programa_1|     2000|            4500|   LAN|         9000000|\n",
      "|Programa_2|     3401|            7000|   LAS|        23807000|\n",
      "|Programa_3|       50|            7000|   LAS|          350000|\n",
      "|Programa_4|     7850|            3300|   USW|        25905000|\n",
      "|Programa_5|     8000|            3505|   LAN|        28040000|\n",
      "+----------+---------+----------------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "nuevo_df = df.withColumn(\"DescargasXLineas\", col(\"Descargas\") * col(\"Lineas_de_codigo\"))\n",
    "nuevo_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos algunas operaciones de `pandas` que tienen un equivalente en `pyspark`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Métodos para imputar faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Método `dropna`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método elimina aquellas filas que contengan entradas nulas. \n",
    "\n",
    "```python\n",
    "DataFrame.dropna(how, thresh=0, subset=DataFrame.columns)\n",
    "```\n",
    "\n",
    "* `how`: <br><br>\n",
    "    * `how = 'any'`: elimina la fila si contiene al menos un faltante.\n",
    "    * `how = 'all'`: elimina la fila si solo contiene faltantes.<br><br>\n",
    "\n",
    "* `thresh`: elimina todas las filas con más datos faltantes que el umbral (de tipo `int`) especificado. <br><br>\n",
    "\n",
    "* `subset`: permite seleccionar un subconjunto de columnas sobre el cual aplicar el método."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, declaramos un `DataFrame` para ejemplificar el uso del método `dropna`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|   A|   B|   C|   D|\n",
      "+----+----+----+----+\n",
      "|   1|null|null|   2|\n",
      "|null|null|   1|null|\n",
      "|null|   0|null|   2|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[1   , None, None, 2   ],\n",
    "        [None, None, 1   , None],\n",
    "        [None, 0   , None, 2   ]]\n",
    "\n",
    "columns = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "df_numeros = spark.createDataFrame(data=data, schema=columns)\n",
    "df_numeros.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el método `dropna` para eliminar aquellas filas que solo contienen datos faltantes en el subconjunto de columnas `A` y `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|   A|   B|   C|   D|\n",
      "+----+----+----+----+\n",
      "|   1|null|null|   2|\n",
      "|null|null|   1|null|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nuevo_df_numeros = df_numeros.dropna(how=\"all\", subset=[\"A\",\"C\"])\n",
    "nuevo_df_numeros.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Método `fillna`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "DataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=0)\n",
    "```\n",
    "\n",
    "* `value`: indica el valor o diccionario de valores para imputar en las entradas nulas.<br><br>\n",
    "\n",
    "* `subset`: permite seleccionar un subconjunto de columnas sobre el cual aplicar el método."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a completar los datos del `DataFrame` `df_numeros`, reemplazando cada valor faltante por un valor predeterminado para su respectiva columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  A|  B|  C|  D|\n",
      "+---+---+---+---+\n",
      "|  1|  0|  1|  2|\n",
      "|  1|  0|  1|  2|\n",
      "|  1|  0|  1|  2|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nuevo_df_numeros = df_numeros.fillna(value={\"A\": 1, \"B\": 0, \"C\": 1, \"D\": 2})\n",
    "nuevo_df_numeros.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Métodos para unir tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Método `join`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrega a un `DataFrame` las columnas de otro según coincidan las columnas especificadas en los dos.\n",
    "\n",
    "```python\n",
    "DataFrame.join(other, on=None, how='inner')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación una explicación de los parámetros.\n",
    "\n",
    "* `other`: el `DataFrame` a unir.<br><br>\n",
    "\n",
    "* `on`: permite usar una o varias columnas de los `DataFrame` para encontrar las coincidencias. Las columnas especificadas deben existir en ambos `DataFrame`.<br><br>\n",
    "\n",
    "* `how`: es `\"inner\"` por defecto. Puedes consultar los posibles valores en la [documentación](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.join.html#pyspark.sql.DataFrame.join), aunque por lo general todas las operaciones `join` tienen la misma nomenclatura independiente del programa de manejo de datos. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos los siguientes `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "|  1|  3|\n",
      "+---+---+\n",
      "\n",
      "+---+---+---+\n",
      "|  B|  C|  D|\n",
      "+---+---+---+\n",
      "|  3|  2|  4|\n",
      "|  3|  4|  1|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2],\n",
    "        [3, 4],\n",
    "        [1, 3]]\n",
    "\n",
    "columns = [\"A\", \"B\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data = data, schema=columns)\n",
    "\n",
    "data = [[3, 2, 4],\n",
    "        [3, 4, 1]]\n",
    "\n",
    "columns = [\"B\", \"C\", \"D\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data = data, schema=columns)\n",
    "\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos agregar a `df` las columnas de `df2` según coincidencia de la columna `\"B\"`, preservando todas las filas de `df` y solo las filas que coinciden de `df2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilicemos el método `join`, especificando que la unión será por coincidencia izquierda externa (que persistan los registros de `df1` y los que coincidan de `df2`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente, podemos utilizar el método `join`. Con este solo necesitamos dos argumentos, puesto que por defecto se usa el índice para la coincidencia de filas de los `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+----+\n",
      "|  B|  A|   C|   D|\n",
      "+---+---+----+----+\n",
      "|  2|  1|null|null|\n",
      "|  4|  3|null|null|\n",
      "|  3|  1|   4|   1|\n",
      "|  3|  1|   2|   4|\n",
      "+---+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = df1.join(df2, on = \"B\", how = 'left')\n",
    "df_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Métodos tipo consulta de SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Método `select`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Método `where`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Método `groupBy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Método `orderBy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cargar datos en `pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para almacenar nuestros datos de manera eficiente y para poder acceder a ellos en el futuro, nos apoyamos en los administradores de bases de datos relacionales (tipo SQL) como SQLite o PostgreSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "SparkBy{Examples}. Spark with Python (PySpark) Tutorial For Beginners. Recuperado el 12 de Agosto de 2022 de: \n",
    "https://sparkbyexamples.com/pyspark-tutorial/\n",
    "\n",
    "Apache PySpark. pyspark.sql.DataFrame. Recuperado el 12 de Agosto de 2022 de: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créditos\n",
    "\n",
    "**Autores:** Alejandro Mantilla Redondo\n",
    "\n",
    "**Fecha última actualización:** 10/07/2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
