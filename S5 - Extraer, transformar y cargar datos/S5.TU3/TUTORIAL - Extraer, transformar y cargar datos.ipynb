{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![miad4.png](Archivos/miad4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción, transformción y carga de datos: `pyspark`\n",
    "\n",
    "<!--# ETL y principios de _Big Data_ ????-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraer, transformar y cargar (ETL por sus siglas en inglés), describe un proceso en tres etapas:\n",
    "\n",
    "1. obtener datos de una o más fuentes; <br>\n",
    "2. transformar los datos, sea haciéndoles limpieza, combinándolos o añadiendo registros; <br>\n",
    "3. grabar los datos, sea cargándolos a su misma fuente, persistiéndolos en archivos locales o alimentando consumidores que dependen de nuestros resultados (aplicaciones _downstream_).\n",
    "\n",
    "Hasta ahora hemos tratado fuentes de datos almacenadas localmente en archivos de texto o formatos de herramientas estadísticas como Stata. La realidad es que los datos existen en un sinfín de contextos, casi siempre, en formatos altamente especializados que exigen procesos especializados de extracción, como SQL (_Structured Query Language_) para bases de datos relacionales. También nos hemos enfocado solo en consumir los datos sin preocuparnos mucho acerca de cómo podemos hacer nuestros resultados disponibles en ocasiones futuras o para otros usuarios.\n",
    "\n",
    "Las bases de datos, como las hemos trabajado, existen únicamente en la memoria volátil de nuestro computador y no persisten de una sesión de Python a la siguiente. Podemos hacer persistir los cambios si los grabamos en archivos tipo `.txt` o `.csv`, pero esto no es manejable a escala o, por ejemplo, cuando queremos desagregar los datos y distribuirlos en distintas tablas para que sean compatibles con operaciones del álgebra relacional (en esencia, lo que hacen las funciones `join` o `merge` de `pandas`).\n",
    "\n",
    "En este tutorial exploraremos el uso de la librería `pyspark` para el manejo de bases de datos relacionales, en el contexto de procesos de ETL para analítica de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requisitos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para desarrollar este tutorial necesitarás:\n",
    "\n",
    "* Importar y exportar archivos de texto en formato `.txt` o `.csv` por medio de un *file handle*. <br>\n",
    "* Utilizar operaciones sencillas y vectorizadas en `numpy` y `pandas`. <br>\n",
    "* Crear, consultar y utilizar métodos para explorar y manipular objetos tipo `DataFrame` en `pandas`. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al final de este tutorial podrás:\n",
    "\n",
    "**1.** Distinguir situaciones en las que resulta más beneficioso utilizar herramientas de ETL como `pyspark`.<br>\n",
    "**2.** Reconocer estructuras de datos que distribuyen la carga de sus operaciones en procesos paralelos.<br>\n",
    "**3.** Extraer y transformar tablas de bases de datos relacionales con operaciones de algebra relacional. <br>\n",
    "**4.** Crear y cargar tablas a bases de datos relacionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Entorno de desarrollo en Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fundación de _software_ Apache es una comunidad dedicada al desarrollo de herramientas _open source_, entre estas, Spark: un motor de procesamiento de datos altamente eficiente. Spark está diseñado para desplegarse en entornos de cómputo distribuido (_cluster_) y paraleliza sus operaciones de manera implícita, lo que lo hace ideal para el procesamiento de volúmenes altos de datos (_Big Data_). Cualquier aplicación puede llegar a imlementar Spark en su flujo de datos por medio de sus APIs (_Application Program Interface_) para distintos lenguajes de programación, como `pyspark` para Python o `SparkR` para R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así como Spark es una aplicación externa a Python a la que accedemos por medio de su API, debemos importar la API, configurar la sesión y luego inicializar la aplicación. Nos enfocaremos en el módulo `sql` para procesos de ETL con bases de datos relacionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 04:13:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://alejoman-pc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Instancia_Tutorial_PySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f909d0779d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos el metodo SparkSession para configurar e inicializar una instancia de Spark SQL\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializamos la aplicacion\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[*]\") \\\n",
    "      .appName(\"Instancia_Tutorial_PySpark\") \\\n",
    "      .getOrCreate()\n",
    "\n",
    "# Imprimimos el objeto que contiene la sesión\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `master` nos permite definir el contexto de paralelización de los procesos. Veamos algunos parámetros que puede recibir:\n",
    "\n",
    "* `\"local[n]\"` le indica a la aplicación que debe ejecutarse en el mismo computador, con `n` cantidad de procesos para paralelizar las tareas; <br><br>\n",
    "\n",
    "* `\"Yarn\"` (u otro nombre de un administrador de _clusters_), le indican a la aplicación que debe ejecutarse en un _cluster_, el cual tendríamos que configurar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al hacer clic en el enlace del _output_ de la celda (Spark UI), podrás acceder a una interfaz web, igual a la de la imágen, con detalles de la aplicación. Esto funciona solo si expones a la red el puerto de la aplicación o si ejecutas el programa en tu propio computador, ya que necesitas acceso al servicio web desplegado por Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark_GUI.png](Archivos/Spark_GUI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estructuras de datos en `pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar la ejecución de tareas en paralelo, Spark incluye dos estructuras de datos que permiten distribuir sus operaciones sobre distintos objetos en memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 RDDs (_Resilient Distributed Dataset_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un RDD es la representación de una colección de objetos que pueden distribuirse en distintos procesos o incluso almacenarse en distintos nodos de nuestro _cluster_. Resultan bastante útiles y eficientes para procesar grandes cantidades de datos, pero resultan poco convenientes si nuestra intención es hacer cambios sobre los datos ya que son inmutables (una vez creados, no pueden editarse).\n",
    "\n",
    "Esta representación es poco restrictiva en cuanto a qué consideramos un objeto de la colección. Para nuestras intenciones, podríamos pensar en un RDD como un `DataFrame` de `pandas` cuyas filas hemos almacenado en distintas variables. Alternativamente, podríamos particionar sobre las columnas, en cuyo caso tendríamos una colección de objetos tipo `Series` o, si una partición toma más de una columna, una colección de objetos tipo `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos declarar un RDD a partir de diferentes estructuras de datos. A continuación, vemos un ejemplo de cómo declarar a partir de una lista de listas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = [[\"Programa_1\", 2000, 4500, \"LAN\"],\n",
    "             [\"Programa_2\", 3401, 7000, \"LAS\"],\n",
    "             [\"Programa_3\", 50,   7000, \"LAS\"],\n",
    "             [\"Programa_4\", 7850, 3300, \"USW\"],\n",
    "             [\"Programa_5\", 8000, 3505, \"LAN\"]]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data_list)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota que al intentar imprimir el RDD, nos encontramos con que la representación en el _output_ de la celda no muestra el contenido. Esto se debe a que las operaciones sobre los RDD son _lazy_ (perezosas) y no se ejecutarán hasta que sea absolutamente necesario. Este esquema de ejecución diferida permite optimizar de manera anticipada las consultas y operaciones para minimizar el tiempo de ejecución y la ocupación de la memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operaciones y consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ahora, basta con saber que hay acciones y transformaciones. Bajo ninguna circunstancia estas operaciones modificarán el RDD original; en su lugar, Spark crea una copia del RDD con los cambios necesarios una vez se hayan realizado las operaciones cargadas de manera perezosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veámos a continuación un ejemplo de cómo operar sobre los elementos de la colección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declaramos una funcion que tome los elementos con indice 1 y 2 de un iterable y los multiplique.\n",
    "def mult(x):\n",
    "    return x[1] * x[2]\n",
    "\n",
    "# La transformacion map toma cada objeto contenido en el RDD y ejecuta la funcion que recibe por parametro.\n",
    "nuevo_rdd = rdd.map(mult)\n",
    "nuevo_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `nuevo_rdd` contiene un RDD distinto a `rdd` para el cual aún no se ha efectuado la transformación. Hacemos uso del método `collect` para ejecutar las operaciones pendientes y recolectar los resultados en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Programa_1', 2000, 4500, 'LAN'],\n",
       " ['Programa_2', 3401, 7000, 'LAS'],\n",
       " ['Programa_3', 50, 7000, 'LAS'],\n",
       " ['Programa_4', 7850, 3300, 'USW'],\n",
       " ['Programa_5', 8000, 3505, 'LAN']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9000000, 23807000, 350000, 25905000, 28040000]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nuevo_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tener que recolectar los objetos de RDD resulta útil en escenarios donde debemos procesar nuestros datos en distintas etapas y no nos interesa el resultado de operaciones intermedias. La ejecución perezosa de las transformaciones nos permite concatenarlas sin ocupar memoria de manera redundante entre operaciones y optimiza de manera automática e inteligente las instrucciones para no repetir tareas redundantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Objeto `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar a los `DataFrame` de `pandas`, los `DataFrame` en `pyspark` son una colección mutable de datos organizados por columnas. A diferencia de `pandas`, que opera de manera secuencial, la implementación de `pyspark` almacena las filas de manera distribuida y opera sobre ellas de manera paralelizada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos declarar un `DataFrame` a partir de diferentes estructuras de datos. A continuación, vemos un ejemplo de cómo declarar a partir de una lista de listas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Nombre: string, Descargas: bigint, Lineas_de_codigo: bigint, Region: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"Nombre\", \"Descargas\", \"Lineas_de_codigo\", \"Region\"]\n",
    "df = spark.createDataFrame(data=data_list, schema=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro `schema` (esquema) hace referencia a los campos de una estructura de datos. En el caso de nuestra lista de listas, el esquema es el nombre de las columnas. Si tuvieramos un diccionario de listas, el esquema serían sus llaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyspark` incluye métodos dedicados a cargar datos desde una gran variedad de formatos. Consideremos, por lo pronto, un format con el que estemos familiarizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- medios_noti_redessociales: string (nullable = true)\n",
      " |-- medios_noti_chat: string (nullable = true)\n",
      " |-- medios_noti_periodicos: string (nullable = true)\n",
      " |-- medios_noti_tv: string (nullable = true)\n",
      " |-- medios_noti_radio: string (nullable = true)\n",
      " |-- medios_covid_redessociales: string (nullable = true)\n",
      " |-- medios_covid_chat: string (nullable = true)\n",
      " |-- medios_covid_periodicos: string (nullable = true)\n",
      " |-- medios_covid_tv: string (nullable = true)\n",
      " |-- medios_covid_radio: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_covid_19 = spark.read.option(\"header\",\"true\").csv(\"Archivos/BID-Cornell.csv\")#, index_col = 0)\n",
    "df_covid_19.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operaciones y consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos distintas alternativas para consultar el contenido de nuestros `DataFrame`. Podemos utilizar el método `show` para imprimir de manera estilizada la tabla o los métodos `head` y `tail` para retornar las primeras o últimas filas de la tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------------+------+\n",
      "|    Nombre|Descargas|Lineas_de_codigo|Region|\n",
      "+----------+---------+----------------+------+\n",
      "|Programa_1|     2000|            4500|   LAN|\n",
      "|Programa_2|     3401|            7000|   LAS|\n",
      "|Programa_3|       50|            7000|   LAS|\n",
      "|Programa_4|     7850|            3300|   USW|\n",
      "|Programa_5|     8000|            3505|   LAN|\n",
      "+----------+---------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Nombre='Programa_1', Descargas=2000, Lineas_de_codigo=4500, Region='LAN'),\n",
       " Row(Nombre='Programa_2', Descargas=3401, Lineas_de_codigo=7000, Region='LAS'),\n",
       " Row(Nombre='Programa_3', Descargas=50, Lineas_de_codigo=7000, Region='LAS')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Nombre='Programa_4', Descargas=7850, Lineas_de_codigo=3300, Region='USW'),\n",
       " Row(Nombre='Programa_5', Descargas=8000, Lineas_de_codigo=3505, Region='LAN')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabe notar que por medio de los métodos `head` y `tail`, obtenemos objetos de tipo `Row` que son los que ditribuye Spark para paralelizar las operaciones. El método `limit` nos permite persitir el tipo `DataFrame` como resultado de la consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------------+------+\n",
      "|    Nombre|Descargas|Lineas_de_codigo|Region|\n",
      "+----------+---------+----------------+------+\n",
      "|Programa_1|     2000|            4500|   LAN|\n",
      "|Programa_2|     3401|            7000|   LAS|\n",
      "|Programa_3|       50|            7000|   LAS|\n",
      "+----------+---------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ocasiones puede ser deseable convertir entre los `DataFrame` de `pyspark` a su contraparte de `pandas`. Transformar un `DataFrame` de `pyspark` a `pandas` es solo recomendable si se cuenta con suficiente almacenamiento o memoria. Los altos volúmenes de datos que manejamos en `pyspark` con gran eficiencia en memoria y procesamiento pueden no ser compatibles con otras librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "withColumn() missing 1 required positional argument: 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDescargas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: withColumn() missing 1 required positional argument: 'col'"
     ]
    }
   ],
   "source": [
    "df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Descargas</th>\n",
       "      <th>Lineas_de_codigo</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Programa_1</td>\n",
       "      <td>2000</td>\n",
       "      <td>4500</td>\n",
       "      <td>LAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Programa_2</td>\n",
       "      <td>3401</td>\n",
       "      <td>7000</td>\n",
       "      <td>LAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Programa_3</td>\n",
       "      <td>50</td>\n",
       "      <td>7000</td>\n",
       "      <td>LAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Programa_4</td>\n",
       "      <td>7850</td>\n",
       "      <td>3300</td>\n",
       "      <td>USW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Programa_5</td>\n",
       "      <td>8000</td>\n",
       "      <td>3505</td>\n",
       "      <td>LAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Nombre  Descargas  Lineas_de_codigo Region\n",
       "0  Programa_1       2000              4500    LAN\n",
       "1  Programa_2       3401              7000    LAS\n",
       "2  Programa_3         50              7000    LAS\n",
       "3  Programa_4       7850              3300    USW\n",
       "4  Programa_5       8000              3505    LAN"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = df.toPandas()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "Referencia 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créditos\n",
    "\n",
    "**Autores:** Alejandro Mantilla Redondo\n",
    "\n",
    "**Fecha última actualización:** 10/07/2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
